{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DVTlrUUXnwoe"
      },
      "outputs": [],
      "source": [
        "!pip install --upgrade transformers accelerate datasets -q\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "id": "yPu6XlHgsCk0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## #preparing the training dataset"
      ],
      "metadata": {
        "id": "FcYgNp77svnv"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "from datasets import Dataset, DatasetDict\n",
        "from sklearn.model_selection import train_test_split\n",
        "\n",
        "\n",
        "df = pd.read_csv(\"/content/drive/MyDrive/LLM_RAG_Learning/project01/data/maintenance_logs.csv\")\n",
        "\n",
        "\n",
        "train_df, eval_df = train_test_split(df, test_size=0.2, random_state=42)\n",
        "\n",
        "train_dataset = Dataset.from_pandas(train_df)\n",
        "eval_dataset = Dataset.from_pandas(eval_df)\n",
        "\n",
        "dataset = DatasetDict({\n",
        "    \"train\": train_dataset,\n",
        "    \"validation\": eval_dataset\n",
        "})\n",
        "\n",
        "dataset\n"
      ],
      "metadata": {
        "id": "rVhifYcSn6ng"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#tokenization"
      ],
      "metadata": {
        "id": "B5Rzs68hs5u6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "model_name = \"google/flan-t5-small\"\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name)\n"
      ],
      "metadata": {
        "id": "xojEn8IKn6ZK"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "max_input_length = 256\n",
        "max_target_length = 64\n",
        "\n",
        "def preprocess_function(examples):\n",
        "    inputs = [\"summarize: \" + text for text in examples[\"log_text\"]]\n",
        "    model_inputs = tokenizer(\n",
        "        inputs,\n",
        "        max_length=max_input_length,\n",
        "        truncation=True\n",
        "    )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            examples[\"summary_text\"],\n",
        "            max_length=max_target_length,\n",
        "            truncation=True\n",
        "        )\n",
        "\n",
        "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
        "    return model_inputs\n",
        "\n",
        "tokenized_datasets = dataset.map(preprocess_function, batched=True)\n",
        "tokenized_datasets\n"
      ],
      "metadata": {
        "id": "vj5om67mn6Hq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from transformers import DataCollatorForSeq2Seq, TrainingArguments, Trainer\n",
        "\n",
        "data_collator = DataCollatorForSeq2Seq(tokenizer=tokenizer, model=model)\n",
        "\n",
        "training_args = TrainingArguments(\n",
        "    output_dir=\"../models/flan_t5_maintenance\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    learning_rate=5e-5,\n",
        "    per_device_train_batch_size=4,\n",
        "    per_device_eval_batch_size=4,\n",
        "    weight_decay=0.01,\n",
        "    save_total_limit=2,\n",
        "    num_train_epochs=5,\n",
        "    logging_steps=10,\n",
        "    push_to_hub=False\n",
        ")\n",
        "\n",
        "trainer = Trainer(\n",
        "    model=model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"validation\"],\n",
        "    tokenizer=tokenizer,\n",
        "    data_collator=data_collator,\n",
        "    compute_metrics=None,  # optional\n",
        ")\n",
        "\n",
        "\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "id": "TpDzUoVStDpF"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def summarize_log(text, max_new_tokens=50):\n",
        "    input_text = \"summarize: \" + text\n",
        "    inputs = tokenizer([input_text], return_tensors=\"pt\", truncation=True)\n",
        "    outputs = model.generate(\n",
        "        **inputs,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        num_beams=4\n",
        "    )\n",
        "    return tokenizer.decode(outputs[0], skip_special_tokens=True)\n",
        "\n",
        "sample = df.iloc[0][\"log_text\"]\n",
        "print(\"LOG:\")\n",
        "print(sample)\n",
        "print(\"\\nMODEL SUMMARY:\")\n",
        "print(summarize_log(sample))\n"
      ],
      "metadata": {
        "id": "bsUmu8uovH93"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.save_pretrained(\"/content/drive/MyDrive/LLM_RAG_Learning/project01/models/flan_t5_maintenance\")\n",
        "tokenizer.save_pretrained(\"/content/drive/MyDrive/LLM_RAG_Learning/project01/models/flan_t5_maintenance\")\n"
      ],
      "metadata": {
        "id": "z81hoZ96CDDv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}